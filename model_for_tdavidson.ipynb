{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertModel, PreTrainedModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "#import pandas as pd\n",
    "from datasets import DatasetDict\n",
    "import preprocessor as p\n",
    "import evaluate\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting the number of threads to 10\n",
      "PyTorch is using 10 threads\n"
     ]
    }
   ],
   "source": [
    "# For efficient usage of the hardware resources when running on JupyterHub EPFL,\n",
    "# we will limit the number of threads. If you are running this code on your local\n",
    "# machine or on colab, the following code will not do anything.\n",
    "if getpass.getuser() == \"jovyan\":\n",
    "    num_threads_limit = 4\n",
    "elif re.search('^https://.*noto.*\\.epfl\\.ch$', os.environ.get(\"EXTERNAL_URL\", \"\")) != None:\n",
    "    num_threads_limit = 2\n",
    "else:\n",
    "    num_threads_limit = torch.get_num_threads()\n",
    "print(f\"Limiting the number of threads to {num_threads_limit}\")\n",
    "torch.set_num_threads(num_threads_limit)\n",
    "print(f\"PyTorch is using {torch.get_num_threads()} threads\")\n",
    "\n",
    "_ = torch.set_flush_denormal(True) # To avoid long training time on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples, max_length=None):\n",
    "    #  padding: 'max_length': pad to a length specified by the max_length argument or the\n",
    "    #  maximum length accepted by the model if no max_length is provided (max_length=None).\n",
    "    #  Padding will still be applied if you only provide a single sequence. [from documentation]\n",
    "\n",
    "\n",
    "    #  truncation: True or 'longest_first': truncate to a maximum length specified\n",
    "    #  by the max_length argument or the maximum length accepted by the model if\n",
    "    #  no max_length is provided (max_length=None). This will truncate token by\n",
    "    #  token, removing a token from the longest sequence in the pair until the\n",
    "    #  proper length is reached. [from documentation]\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.strip()\n",
    "\n",
    "    #text = p.clean(text)\n",
    "    \n",
    "    # Remove quotation marks at the beginning of the string\n",
    "    text= re.sub(r'^!{1,}', '', text)\n",
    "\n",
    "    # Remove @names\n",
    "    text = re.sub(r'@[^ ]+:', \"user: \", text)\n",
    "\n",
    "    # Remove @names\n",
    "    text = re.sub(r'@[^ ]+\\s:', \"user: \", text)\n",
    "    \n",
    "    # Remove \"RT\" elements\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "\n",
    "    # Remove multiple whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove \"&#number\" elements\n",
    "    text = re.sub(r'&#[0-9]+', '', text)\n",
    "    \n",
    "    #Remove hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove multiple exclamation marks\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "\n",
    "    #Normalize\n",
    "    text= re.sub(r'([A-Za-z])\\1{2,}', r'\\1', text)\n",
    "\n",
    "    # Remove &amp;\n",
    "    text = re.sub(r'&amp;', 'and', text)\n",
    "\n",
    "    text=re.sub(r'https?://[^ ]+', '', text)\n",
    "\n",
    "    text=re.sub(r'&;', '', text)\n",
    "\n",
    "    text = re.sub(r'&lt;', ' ', text)\n",
    "\n",
    "    text = re.sub(r'^:\\s', '', text)\n",
    "\n",
    "    text = re.sub(r';', '', text)\n",
    "\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    text = p.clean(text)\n",
    "\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def split_dataset(data):\n",
    "    dataset = data.train_test_split(test_size=0.2, shuffle=True, stratify_by_column=\"class\")\n",
    "    dataset_train = dataset[\"train\"]\n",
    "    test_vali = dataset[\"test\"].train_test_split(test_size=0.5, shuffle=True, stratify_by_column=\"class\")\n",
    "    dataset_test = test_vali[\"test\"]\n",
    "    dataset_validation = test_vali[\"train\"]\n",
    "\n",
    "    dataset_dict = DatasetDict({\"train\": dataset_train,\"validation\": dataset_validation,\"test\": dataset_test})\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "def get_dataloader(dataset=None,batch_size=16, max_length=512):\n",
    "    if dataset is None:\n",
    "        dataset=load_from_disk(\"path/to/dataset\")\n",
    "    \n",
    "    tokenized_dataset = dataset.rename_column(\"tweet\", \"text\")\n",
    "    tokenized_dataset=tokenized_dataset.rename_column(\"class\", \"labels\")\n",
    "    tokenized_dataset= tokenized_dataset.remove_columns(['count', 'hate_speech_count', 'offensive_language_count', 'neither_count'])\n",
    "    tokenized_dataset=tokenized_dataset.map(\n",
    "        lambda examples: tokenize_function(examples, max_length=max_length),\n",
    "        batched=True\n",
    "    )\n",
    "    columns_to_keep = ['labels', 'input_ids', 'attention_mask']\n",
    "    tokenized_dataset = tokenized_dataset.select_columns(columns_to_keep)\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=batch_size)\n",
    "    eval_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, eval_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"tdavidson/hate_speech_offensive\")\n",
    "data = data[\"train\"]\n",
    "data = data.map(\n",
    "    lambda examples: {'tweet': preprocess_text(examples['tweet'])}\n",
    ")\n",
    "dataset_dict = split_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64ccc093ab2423f9580bf5491046658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf9a9837b3d4805bfc790d45f00b51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2478 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3337d88c20046888e8fe340c5230805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader, eval_dataloader, test_dataloader=get_dataloader(dataset_dict, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClassifierDavidsonDataset(PreTrainedModel):\n",
    "    def __init__(self, config, weights=None):\n",
    "        super(DistilBERTClassifierDavidsonDataset, self).__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        \n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        self.lin_class_1 = nn.Linear(config.hidden_size, config.dim)\n",
    "        self.lin_class_2 = nn.Linear(config.dim, config.num_labels)\n",
    "        \n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # for param in self.distilbert.transformer.layer[-3:].parameters():\n",
    "        #     param.requires_grad = True\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = torch.ones(self.num_labels)\n",
    "        self.weights = weights\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        device = distilbert_output.last_hidden_state.device\n",
    "        \n",
    "        pooled_output = distilbert_output.last_hidden_state[:, 0, :]\n",
    "        classifier_output = self.lin_class_1(pooled_output)\n",
    "        classifier_output = nn.ReLU()(classifier_output)#+classifier_output\n",
    "        classifier_output = self.dropout(classifier_output)\n",
    "        logits = self.lin_class_2(classifier_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.weights.to(device))\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        # if not return_dict:\n",
    "        #     output = (logits,) + distilbert_output[1:]\n",
    "        #     return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=distilbert_output.hidden_states,\n",
    "            attentions=distilbert_output.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(dataloader, num_classes):\n",
    "    class_counts = torch.zeros(num_classes)\n",
    "    \n",
    "    # Iterate through the dataset and count occurrences of each class\n",
    "    for batch in dataloader:\n",
    "        labels = batch[\"labels\"]\n",
    "        class_counts += torch.bincount(labels, minlength=num_classes)\n",
    "    \n",
    "    # Compute class frequencies\n",
    "    class_frequencies = class_counts / class_counts.sum()\n",
    "\n",
    "    # Compute class weights as the inverse of class frequencies\n",
    "    class_weights = 1.0 / class_frequencies\n",
    "\n",
    "    # Normalize weights to sum to 1.0\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "weights = compute_class_weights(train_dataloader, 3)\n",
    "config = DistilBertConfig(\n",
    "    num_labels=3,  # Assuming you have three classes\n",
    "    hidden_size=768,\n",
    "    dim=768,  # Adjust dimensionality as needed\n",
    "    dropout=0.3,  # Adjust dropout rate as needed\n",
    ")\n",
    "\n",
    "model = DistilBERTClassifierDavidsonDataset(config, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(dataloader, num_classes):\n",
    "    class_counts = torch.zeros(num_classes)\n",
    "    \n",
    "    # Iterate through the dataset and count occurrences of each class\n",
    "    for batch in dataloader:\n",
    "        labels = batch[\"labels\"]\n",
    "        class_counts += torch.bincount(labels, minlength=num_classes)\n",
    "    \n",
    "    # Compute class frequencies\n",
    "    class_frequencies = class_counts / class_counts.sum()\n",
    "\n",
    "    # Compute class weights as the inverse of class frequencies\n",
    "    class_weights = 1.0 / class_frequencies\n",
    "\n",
    "    # Normalize weights to sum to 1.0\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "    return class_weights, class_counts, class_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    load_accuracy = evaluate.load(\"accuracy\")\n",
    "    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"test_graphs\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=1e-5,\n",
    "   per_device_train_batch_size=batch_size,\n",
    "   per_device_eval_batch_size=batch_size,\n",
    "   num_train_epochs=4,\n",
    "   weight_decay=0.05,\n",
    "   save_strategy=\"epoch\",\n",
    "   push_to_hub=False,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   warmup_steps=1000,\n",
    "   #lr_scheduler_type=\"constant\",\n",
    "   #gradient_accumulation_steps=4,  # Accumulate gradients for every 4 steps\n",
    "   fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=train_dataloader.dataset,\n",
    "   eval_dataset=test_dataloader.dataset,\n",
    "   data_collator=None,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aff91daacd45bca123388b9e01b0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b8062d5deb4c1b9f07a62f42f7f4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2645208239555359, 'eval_accuracy': 0.903954802259887, 'eval_precision': 0.8919929550296073, 'eval_recall': 0.903954802259887, 'eval_f1': 0.8966148915159169, 'eval_runtime': 14.1044, 'eval_samples_per_second': 175.689, 'eval_steps_per_second': 2.765, 'epoch': 1.0}\n",
      "{'loss': 0.2218, 'grad_norm': 2.832308292388916, 'learning_rate': 5e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cf1ad35f7e47c28f95013e5573bae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2663116157054901, 'eval_accuracy': 0.9003228410008071, 'eval_precision': 0.8852230794742849, 'eval_recall': 0.9003228410008071, 'eval_f1': 0.8903384606474465, 'eval_runtime': 14.2527, 'eval_samples_per_second': 173.861, 'eval_steps_per_second': 2.736, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9209949d50404756a657911509835097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2833469808101654, 'eval_accuracy': 0.8962873284907183, 'eval_precision': 0.8859859017080626, 'eval_recall': 0.8962873284907183, 'eval_f1': 0.8897405564856194, 'eval_runtime': 14.1259, 'eval_samples_per_second': 175.422, 'eval_steps_per_second': 2.761, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1858\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1856\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\guill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2207\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2202\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2205\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2208\u001b[0m ):\n\u001b[0;32m   2209\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2210\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bb5518a357437086ccf64ff4b0e34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.23490408062934875,\n",
       " 'eval_accuracy': 0.9169019766034692,\n",
       " 'eval_precision': 0.907462164601783,\n",
       " 'eval_recall': 0.9169019766034692,\n",
       " 'eval_f1': 0.9100991719540924,\n",
       " 'eval_runtime': 20.8073,\n",
       " 'eval_samples_per_second': 119.141,\n",
       " 'eval_steps_per_second': 1.874}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8021e77829fb44a8bc2d1e2caad139c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24347041547298431,\n",
       " 'eval_accuracy': 0.9120613150463897,\n",
       " 'eval_precision': 0.9040360649517097,\n",
       " 'eval_recall': 0.9120613150463897,\n",
       " 'eval_f1': 0.9048665982550499,\n",
       " 'eval_runtime': 13.3986,\n",
       " 'eval_samples_per_second': 185.02,\n",
       " 'eval_steps_per_second': 2.911,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset=test_dataloader.dataset\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoint-1240'\n",
    "model = DistilBERTClassifierDavidsonDataset.from_pretrained(model_path, config=config, weights=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
